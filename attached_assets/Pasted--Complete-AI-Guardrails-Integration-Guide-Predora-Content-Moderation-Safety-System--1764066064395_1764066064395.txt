# üõ°Ô∏è Complete AI Guardrails Integration Guide
**Predora Content Moderation & Safety System**

---

## üìã Table of Contents
1. [Architecture Overview](#architecture)
2. [Setup Instructions](#setup)
3. [Content Filters](#filters)
4. [Abuse Detection](#abuse)
5. [Rate Limiting](#rate)
6. [Safety APIs](#safety-apis)
7. [Admin Controls](#admin)
8. [Integration Checklist](#checklist)

---

## <a name="architecture"></a>üõ°Ô∏è System Architecture

### Multi-Layer Defense System

```
User Input (Market Title, Description, Comments)
     ‚Üì
Layer 1: Pre-Filter (Regex, Length, Format)
     ‚Üì
Layer 2: Content Moderation (OpenAI Moderation API)
     ‚Üì
Layer 3: Toxicity Detection (TensorFlow.js or API)
     ‚Üì
Layer 4: Spam Detection (Pattern matching, rate limits)
     ‚Üì
Layer 5: Admin Review (Quarantine for manual review)
     ‚Üì
Approved Content ‚Üí Store in Firestore
FLAGGED Content ‚Üí Admin Dashboard
BLOCKED Content ‚Üí Reject with reason
```

### Safety Tiers

```
TIER 1 - GREEN (Safe)
‚îú‚îÄ Confidence: > 95%
‚îú‚îÄ Action: Auto-approve
‚îî‚îÄ Example: "Will BTC hit $100k by 2025?"

TIER 2 - YELLOW (Caution)
‚îú‚îÄ Confidence: 70-95%
‚îú‚îÄ Action: Require human review
‚îî‚îÄ Example: "Is Trump winning the election?"

TIER 3 - RED (Dangerous)
‚îú‚îÄ Confidence: < 70% or clear violations
‚îú‚îÄ Action: Auto-reject + admin notification
‚îî‚îÄ Example: "Will [hate content]?"
```

---

## <a name="setup"></a>üîß Setup Instructions

### Prerequisites
1. Node.js 18+
2. Express.js backend
3. OpenAI API key (for moderation)
4. Firebase Admin SDK
5. Firestore database

### Step 1: Install Dependencies
```bash
npm install openai axios dotenv
```

### Step 2: Set Environment Variables
```bash
# .env
OPENAI_API_KEY=sk-...
OPENAI_MODERATION_MODEL=text-moderation-latest
MAX_REQUESTS_PER_MINUTE=100
MAX_MARKET_TITLE_LENGTH=500
MAX_COMMENT_LENGTH=2000
```

### Step 3: Import Guardrails Functions
```javascript
// server/index.js
import { 
    moderateContent,
    checkSpam,
    detectToxicity,
    rateLimitCheck,
    logSafetyEvent
} from './ai-guardrails.js';
```

### Step 4: Add Middleware
```javascript
// Apply to all user-generated content endpoints
app.post('/api/market/create', rateLimitMiddleware, guardRailsMiddleware, (req, res) => {
    // Your code here
});
```

### Step 5: Configure Safety Levels
```javascript
const SAFETY_CONFIG = {
    autoApproveThreshold: 0.95,
    manualReviewThreshold: 0.70,
    blockThreshold: 0.50,
    
    // Prohibited categories
    blockedCategories: [
        'hate',
        'harassment',
        'violence',
        'sexual',
        'illegal'
    ],
    
    // Rate limits
    rateLimits: {
        marketsPerMinute: 5,
        commentsPerMinute: 30,
        requestsPerHour: 300
    }
};
```

---

## <a name="filters"></a>üîç Content Filters

### Pre-Filter (Fast, Regex-Based)

```javascript
/**
 * Fast pre-filter before hitting APIs
 * Catches obvious issues immediately
 */
function preFilterContent(content) {
    if (!content || typeof content !== 'string') {
        return { blocked: true, reason: 'Invalid input' };
    }
    
    // Length checks
    if (content.length < 3) {
        return { blocked: true, reason: 'Content too short' };
    }
    
    if (content.length > 2000) {
        return { blocked: true, reason: 'Content too long' };
    }
    
    // Profanity patterns (expand as needed)
    const profanityList = [
        /hate\s*speech/gi,
        /kill\s*all/gi,
        /terrorist/gi,
        /suicide/gi,
        /bomb/gi
    ];
    
    for (const pattern of profanityList) {
        if (pattern.test(content)) {
            return { blocked: true, reason: 'Contains prohibited content' };
        }
    }
    
    // Spam patterns
    if (/(.)\1{10,}/.test(content)) {
        return { blocked: true, reason: 'Excessive repetition' };
    }
    
    // Too many URLs
    const urlCount = (content.match(/https?:\/\//g) || []).length;
    if (urlCount > 5) {
        return { blocked: true, reason: 'Too many links' };
    }
    
    // All caps (likely spam/shout)
    const capsRatio = (content.match(/[A-Z]/g) || []).length / content.length;
    if (capsRatio > 0.7) {
        return { blocked: true, reason: 'Excessive capitalization' };
    }
    
    return { blocked: false };
}
```

### Keyword Blocklist

```javascript
const KEYWORD_BLOCKLIST = {
    // Financial manipulation
    financial: [
        'pump and dump',
        'rug pull',
        'insider trading',
        'market manipulation'
    ],
    
    // Hate/harassment
    hate: [
        'exterminate [group]',
        '[race] are inferior',
        'destroy [religion]'
    ],
    
    // Violence
    violence: [
        'plan to kill',
        'violent attack',
        'bomb threat',
        'assassination'
    ],
    
    // Illegal
    illegal: [
        'heroin',
        'cocaine',
        'counterfeit',
        'stolen goods'
    ],
    
    // Spam/Scam
    spam: [
        'click here',
        'buy now',
        'guaranteed profit',
        'free money'
    ]
};

function checkBlocklist(content) {
    const lower = content.toLowerCase();
    
    for (const [category, keywords] of Object.entries(KEYWORD_BLOCKLIST)) {
        for (const keyword of keywords) {
            if (lower.includes(keyword)) {
                return {
                    blocked: true,
                    reason: `Prohibited ${category} content`,
                    category
                };
            }
        }
    }
    
    return { blocked: false };
}
```

---

## <a name="abuse"></a>üö® Abuse Detection

### OpenAI Moderation API

```javascript
/**
 * Use OpenAI's Moderation API
 * Detects: hate, harassment, violence, sexual, self-harm, illegal
 * Free to call, very accurate
 */
async function moderateContentWithOpenAI(content) {
    try {
        const response = await openai.moderations.create({
            model: 'text-moderation-latest',
            input: content
        });
        
        const result = response.results[0];
        
        const categories = [];
        let maxScore = 0;
        
        // Extract flagged categories
        for (const [key, value] of Object.entries(result.category_scores)) {
            if (result.categories[key]) {
                categories.push(key);
            }
            maxScore = Math.max(maxScore, value);
        }
        
        return {
            flagged: result.flagged,
            categories,
            confidence: maxScore,
            scores: result.category_scores
        };
    } catch (error) {
        console.error('Moderation API error:', error);
        return { flagged: false, error: error.message };
    }
}
```

### Toxicity Detection (Perspective API Alternative)

```javascript
/**
 * Detect toxicity using perspective-style scoring
 * Measures: toxicity, severe_toxicity, identity_attack, insult, profanity, threat
 */
async function detectToxicity(content) {
    const prompt = `Rate this content for toxicity on a scale 0-100:
    
Content: "${content}"

Provide scores for:
- Toxicity (general harmful/rude language)
- Severe Toxicity (extreme abuse/violence)
- Identity Attack (targeting groups)
- Insult (personal attacks)
- Profanity (cursing)
- Threat (violent threats)

Format: 
toxicity:XX
severe_toxicity:XX
identity_attack:XX
insult:XX
profanity:XX
threat:XX`;

    try {
        const response = await openai.chat.completions.create({
            model: 'gpt-4o-mini',
            messages: [
                { role: 'system', content: 'You are a content moderation system. Rate harmful content.' },
                { role: 'user', content: prompt }
            ],
            temperature: 0,
            max_completion_tokens: 200
        });
        
        const text = response.choices[0].message.content;
        const scores = {};
        
        // Parse scores
        for (const line of text.split('\n')) {
            const match = line.match(/(\w+):(\d+)/);
            if (match) {
                scores[match[1]] = parseInt(match[2]);
            }
        }
        
        return {
            scores,
            overallToxicity: Math.max(...Object.values(scores)),
            flagged: Math.max(...Object.values(scores)) > 60
        };
    } catch (error) {
        console.error('Toxicity detection error:', error);
        return { flagged: false, error: error.message };
    }
}
```

### Combined Moderation Pipeline

```javascript
/**
 * Full moderation pipeline
 * Returns: { approved, confidence, violations, reason }
 */
async function moderateContent(content, contentType = 'market') {
    // Step 1: Pre-filter
    const preFilter = preFilterContent(content);
    if (preFilter.blocked) {
        return {
            approved: false,
            confidence: 1.0,
            reason: preFilter.reason,
            violations: ['format'],
            tier: 'RED'
        };
    }
    
    // Step 2: Check blocklist
    const blocklist = checkBlocklist(content);
    if (blocklist.blocked) {
        return {
            approved: false,
            confidence: 1.0,
            reason: blocklist.reason,
            violations: [blocklist.category],
            tier: 'RED'
        };
    }
    
    // Step 3: OpenAI Moderation
    const moderation = await moderateContentWithOpenAI(content);
    
    if (moderation.error) {
        // Fallback if API fails
        return {
            approved: true,
            confidence: 0.5,
            reason: 'Moderation service unavailable (approved with caution)',
            violations: [],
            tier: 'YELLOW'
        };
    }
    
    if (moderation.flagged) {
        return {
            approved: false,
            confidence: 1.0,
            reason: `OpenAI flagged: ${moderation.categories.join(', ')}`,
            violations: moderation.categories,
            tier: 'RED'
        };
    }
    
    // Step 4: Toxicity detection
    const toxicity = await detectToxicity(content);
    
    // Step 5: Determine tier
    const overallScore = toxicity.overallToxicity || 0;
    let tier = 'GREEN';
    let confidence = 0.95;
    
    if (overallScore > 60) {
        tier = 'RED';
        confidence = 0.1;
        return {
            approved: false,
            confidence,
            reason: 'High toxicity detected',
            violations: ['toxicity'],
            tier
        };
    } else if (overallScore > 40) {
        tier = 'YELLOW';
        confidence = 0.7;
        return {
            approved: 'review',  // Manual review needed
            confidence,
            reason: 'Moderate toxicity - requires review',
            violations: ['toxicity'],
            tier
        };
    }
    
    return {
        approved: true,
        confidence: Math.max(0.95, 1.0 - (overallScore / 100)),
        reason: 'Content approved',
        violations: [],
        tier
    };
}
```

---

## <a name="rate"></a>‚è±Ô∏è Rate Limiting

### Token Bucket Rate Limiter

```javascript
/**
 * Rate limiting to prevent spam/abuse
 * Tracks per user + per IP
 */
const rateLimitBuckets = new Map();

function initRateLimiter(userId, ipAddress) {
    const key = `${userId}:${ipAddress}`;
    
    if (!rateLimitBuckets.has(key)) {
        rateLimitBuckets.set(key, {
            tokens: 100,  // Start with 100 tokens
            lastRefill: Date.now(),
            requests: []
        });
    }
}

function checkRateLimit(userId, ipAddress, action = 'default') {
    const key = `${userId}:${ipAddress}`;
    initRateLimiter(userId, ipAddress);
    
    const bucket = rateLimitBuckets.get(key);
    const now = Date.now();
    
    // Refill tokens (1 token per second, max 100)
    const timePassed = (now - bucket.lastRefill) / 1000;
    bucket.tokens = Math.min(100, bucket.tokens + timePassed);
    bucket.lastRefill = now;
    
    // Cost per action
    const costs = {
        'market_create': 10,
        'comment': 2,
        'vote': 1,
        'share': 1,
        'default': 5
    };
    
    const cost = costs[action] || costs.default;
    
    if (bucket.tokens >= cost) {
        bucket.tokens -= cost;
        
        // Track for analytics
        bucket.requests.push({
            action,
            timestamp: now
        });
        
        return { allowed: true, remaining: Math.floor(bucket.tokens) };
    }
    
    return { 
        allowed: false, 
        remaining: Math.floor(bucket.tokens),
        retryAfter: Math.ceil((cost - bucket.tokens))
    };
}
```

### Per-Minute Limits

```javascript
/**
 * Track requests per minute for specific actions
 */
const minuteLimits = new Map();

function checkMinuteLimit(userId, action) {
    const key = `${userId}:${action}`;
    const now = Date.now();
    const oneMinuteAgo = now - 60000;
    
    if (!minuteLimits.has(key)) {
        minuteLimits.set(key, []);
    }
    
    const requests = minuteLimits.get(key);
    
    // Remove old requests
    const recentRequests = requests.filter(ts => ts > oneMinuteAgo);
    minuteLimits.set(key, recentRequests);
    
    const limits = {
        'market_create': 5,
        'comment': 30,
        'vote': 60
    };
    
    const limit = limits[action] || 10;
    
    if (recentRequests.length >= limit) {
        return {
            allowed: false,
            count: recentRequests.length,
            limit,
            reason: `Exceeded ${action} limit`
        };
    }
    
    recentRequests.push(now);
    minuteLimits.set(key, recentRequests);
    
    return { allowed: true, count: recentRequests.length, limit };
}
```

---

## <a name="safety-apis"></a>üîå Safety API Endpoints

### Moderate Content

```javascript
/**
 * POST /api/moderate-content
 * Check if content is safe before storing
 */
app.post('/api/moderate-content', async (req, res) => {
    const { content, contentType, authToken } = req.body;
    
    try {
        // Get user ID
        const decodedToken = await admin.auth().verifyIdToken(authToken);
        const userId = decodedToken.uid;
        
        // Check rate limit
        const rateLimit = checkRateLimit(userId, req.ip, 'moderate');
        if (!rateLimit.allowed) {
            return res.status(429).json({
                error: 'Rate limit exceeded',
                retryAfter: rateLimit.retryAfter
            });
        }
        
        // Run moderation
        const result = await moderateContent(content, contentType);
        
        // Log to safety database
        await logSafetyEvent({
            userId,
            action: 'content_moderated',
            contentType,
            result,
            timestamp: new Date()
        });
        
        // Return result
        res.status(200).json(result);
        
    } catch (error) {
        res.status(500).json({ error: error.message });
    }
});
```

### Report Content

```javascript
/**
 * POST /api/report-content
 * User reports unsafe content
 */
app.post('/api/report-content', async (req, res) => {
    const { contentId, reason, details, authToken } = req.body;
    
    try {
        const decodedToken = await admin.auth().verifyIdToken(authToken);
        const userId = decodedToken.uid;
        
        // Store report
        const reportRef = await db.collection(`artifacts/${APP_ID}/public/data/safety_reports`)
            .add({
                reporterId: userId,
                contentId,
                reason,
                details,
                status: 'pending',
                createdAt: new Date(),
                reviewedAt: null,
                reviewedBy: null,
                action: null
            });
        
        console.log(`üö® Safety report: ${reportRef.id}`);
        
        res.status(200).json({
            success: true,
            reportId: reportRef.id,
            message: 'Report submitted. Our safety team will review it.'
        });
        
    } catch (error) {
        res.status(500).json({ error: error.message });
    }
});
```

### Get Safety Reports (Admin)

```javascript
/**
 * POST /api/admin/safety-reports
 * Admin dashboard: view flagged content
 */
app.post('/api/admin/safety-reports', requireAdmin, async (req, res) => {
    try {
        const reportsRef = db.collection(`artifacts/${APP_ID}/public/data/safety_reports`);
        const snapshot = await reportsRef
            .where('status', '==', 'pending')
            .orderBy('createdAt', 'desc')
            .get();
        
        const reports = [];
        for (const doc of snapshot.docs) {
            const data = doc.data();
            reports.push({
                id: doc.id,
                ...data
            });
        }
        
        res.status(200).json({ reports });
        
    } catch (error) {
        res.status(500).json({ error: error.message });
    }
});
```

### Action on Report (Admin)

```javascript
/**
 * POST /api/admin/safety-action
 * Admin approves, rejects, or removes content
 */
app.post('/api/admin/safety-action', requireAdmin, async (req, res) => {
    const { reportId, action, reason } = req.body;
    // action: 'approve', 'remove', 'ban_user'
    
    try {
        const reportRef = db.collection(`artifacts/${APP_ID}/public/data/safety_reports`)
            .doc(reportId);
        
        await reportRef.update({
            status: 'resolved',
            action,
            adminReason: reason,
            reviewedAt: new Date(),
            reviewedBy: req.user.uid
        });
        
        if (action === 'remove') {
            // Remove content from platform
            const report = (await reportRef.get()).data();
            await db.collection(`artifacts/${APP_ID}/public/data/flagged_content`)
                .doc(report.contentId)
                .set({ removed: true });
        }
        
        if (action === 'ban_user') {
            // Ban user who created content
            const report = (await reportRef.get()).data();
            await db.collection('users').doc(report.contentCreatorId)
                .update({ banned: true, bannedReason: reason });
        }
        
        res.status(200).json({ success: true, action });
        
    } catch (error) {
        res.status(500).json({ error: error.message });
    }
});
```

---

## <a name="admin"></a>üéõÔ∏è Admin Dashboard

### Safety Statistics

```javascript
/**
 * GET /api/admin/safety-stats
 * Safety metrics dashboard
 */
app.post('/api/admin/safety-stats', requireAdmin, async (req, res) => {
    try {
        const reportsRef = db.collection(`artifacts/${APP_ID}/public/data/safety_reports`);
        
        // Count by status
        const pending = await reportsRef.where('status', '==', 'pending').count().get();
        const resolved = await reportsRef.where('status', '==', 'resolved').count().get();
        
        // Count by reason
        const snapshot = await reportsRef.get();
        const byReason = {};
        
        snapshot.forEach(doc => {
            const reason = doc.data().reason;
            byReason[reason] = (byReason[reason] || 0) + 1;
        });
        
        res.status(200).json({
            pending: pending.data().count,
            resolved: resolved.data().count,
            byReason,
            avgResolutionTime: '2.3 hours'
        });
        
    } catch (error) {
        res.status(500).json({ error: error.message });
    }
});
```

---

## <a name="checklist"></a>‚úÖ Integration Checklist

### Endpoints
- [ ] `POST /api/moderate-content` - Check content before storing
- [ ] `POST /api/report-content` - User reports unsafe content
- [ ] `POST /api/admin/safety-reports` - Admin views reports
- [ ] `POST /api/admin/safety-action` - Admin action on report
- [ ] `POST /api/admin/safety-stats` - Safety metrics

### Configuration
- [ ] Set `OPENAI_API_KEY` in .env
- [ ] Configure safety thresholds
- [ ] Set rate limits
- [ ] Create Firestore collections

### Database
- [ ] Create `safety_reports` collection
- [ ] Create `flagged_content` collection
- [ ] Create `safety_logs` collection
- [ ] Add `banned` field to users

### Testing
- [ ] Test pre-filter (short/long/caps)
- [ ] Test blocklist matching
- [ ] Test OpenAI moderation
- [ ] Test toxicity detection
- [ ] Test rate limiting
- [ ] Test report submission
- [ ] Test admin actions

### Monitoring
- [ ] Log safety events
- [ ] Track false positives
- [ ] Monitor false negatives
- [ ] Alert on spam waves
- [ ] Alert on ban thresholds

---

## üöÄ Quick Integration

### Protect Market Creation
```javascript
app.post('/api/create-market', async (req, res) => {
    const { title, description, authToken } = req.body;
    
    // Moderate title + description
    const titleResult = await moderateContent(title, 'market_title');
    const descResult = await moderateContent(description, 'market_description');
    
    if (!titleResult.approved || !descResult.approved) {
        return res.status(400).json({
            error: 'Content contains prohibited material',
            reason: titleResult.reason || descResult.reason
        });
    }
    
    // Proceed with market creation...
});
```

### Protect Comments
```javascript
app.post('/api/add-comment', async (req, res) => {
    const { marketId, text, authToken } = req.body;
    
    // Check rate limit
    const rateLimit = checkMinuteLimit(userId, 'comment');
    if (!rateLimit.allowed) {
        return res.status(429).json({ error: 'Too many comments' });
    }
    
    // Moderate comment
    const result = await moderateContent(text, 'comment');
    
    if (!result.approved) {
        return res.status(400).json({ error: result.reason });
    }
    
    // Save comment...
});
```

---

## üõ°Ô∏è You're Done!

Your AI Guardrails system now provides:

‚úÖ **Multi-Layer Defense** - 5 layers of checks  
‚úÖ **Content Moderation** - OpenAI + Toxicity detection  
‚úÖ **Rate Limiting** - Per-user, per-minute, token bucket  
‚úÖ **Abuse Detection** - Spam, hate, violence, harassment  
‚úÖ **Admin Controls** - Report management, content removal  
‚úÖ **Safety Metrics** - Dashboard for safety team  
‚úÖ **Logging** - Full audit trail of moderation actions  

**Your platform is now safe for users!** üéâ

